{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation\n",
    "\n",
    "This project is focused on the development of an AI translator using Recurrent Neural Networks (RNNs). The objective is to design and train a Seq2Seq architecture using Pytorch, a popular deep learning library, to translate text from one language to another. Specifically, the project uses the Multi30k dataset, which contains approximately 30,000 sentences in English, German, and French.\n",
    "\n",
    "The AI translator is designed using an encoder-decoder architecture, where the encoder takes the input sentence and produces a fixed-length representation (encoding) of the input sequence. The decoder then uses this encoding to generate the output sentence in the target language. The RNNs, specifically LSTM (Long Short-Term Memory) networks, are used in both the encoder and decoder to capture the temporal dependencies in the input and output sequences.\n",
    "\n",
    "The intended audience for this project is anyone interested in natural language processing and deep learning, particularly those interested in machine translation. \n",
    "\n",
    "Note: This project is designed as an educational project and should not be used for any commercial or production purposes.\n",
    "\n",
    "### Step 1: Build the Vocabulary & create the Word Embeddings\n",
    "* The most important part of this step is to create your Vocabulary object using a corpus of data drawn from TorchText.\n",
    "* Use NLTK to create a function to tokenize the text and look up the index of a word's embeddings.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install numpy==1.16.5\n",
    "! pip install torch==1.3.1\n",
    "! pip install torchtext==0.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from typing import List\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')  # download the 'punkt' package from the Natural Language Toolkit (NLTK) library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define two functions to tokenize German and English text data, respectively.\n",
    "# The 'nltk.word_tokenize' function is used to split the text into individual words.\n",
    "# The '[::-1]' slice is used to reverse the order of the words in the German text.\n",
    "def tokenize_de(text: str) -> List[str]:\n",
    "    return nltk.word_tokenize(text, language='german')[::-1]\n",
    "\n",
    "def tokenize_en(text: str) -> List[str]:\n",
    "    return nltk.word_tokenize(text, language='english')\n",
    "\n",
    "# Define two fields to represent the source and target text data, respectively.\n",
    "# The 'tokenize' argument specifies the function to use for tokenizing the text data.\n",
    "# The 'init_token' and 'eos_token' arguments are used to add special tokens to the beginning and end of each sequence.\n",
    "# The 'lower' argument is used to convert all text to lowercase.\n",
    "source = Field(tokenize=tokenize_de, init_token='<sos>', eos_token='<eos>', lower=True)\n",
    "target = Field(tokenize=tokenize_en, init_token='<sos>', eos_token='<eos>', lower=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 29000\n",
      "Number of validation examples: 1014\n",
      "Number of testing examples: 1000\n"
     ]
    }
   ],
   "source": [
    "# Load the Multi30k dataset, which contains parallel German-English text data.\n",
    "train_data, valid_data, test_data = Multi30k.splits(exts=('.de', '.en'), fields=(source, target))\n",
    "\n",
    "# Print the number of examples in each dataset.\n",
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_data.examples)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in source (de) vocabulary: 7860\n",
      "Unique tokens in target (en) vocabulary: 5920\n"
     ]
    }
   ],
   "source": [
    "# Build the vocabulary for the source and target fields using the training data.\n",
    "# The 'min_freq' argument specifies the minimum frequency a word must have to be included in the vocabulary.\n",
    "source.build_vocab(train_data, min_freq=2)\n",
    "target.build_vocab(train_data, min_freq=2)\n",
    "\n",
    "# Print the number of unique tokens in the source and target vocabularies.\n",
    "print(f\"Unique tokens in source (de) vocabulary: {len(source.vocab)}\")\n",
    "print(f\"Unique tokens in target (en) vocabulary: {len(target.vocab)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the batch size and device for processing the data.\n",
    "BATCH_SIZE = 128\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Construct the iterator objects for the training, validation, and test data.\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), batch_size=BATCH_SIZE, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the hyperparameters for the model.\n",
    "\n",
    "# 'INPUT_DIM' and 'OUTPUT_DIM' specify the sizes of the input and output vocabularies.\n",
    "# 'EMBEDDING_DIM' specifies the size of the word embeddings.\n",
    "# 'HIDDEN_DIM' specifies the size of the hidden state of the LSTM cells.\n",
    "# 'NUM_LAYERS' specifies the number of layers in the LSTM cells.\n",
    "# 'DROPOUT' specifies the dropout probability for the output layer.\n",
    "\n",
    "INPUT_DIM = len(source.vocab)\n",
    "OUTPUT_DIM = len(target.vocab)\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.5\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create the Encoder\n",
    "* A Seq2Seq architecture consists of an encoder and a decoder unit. We will use Pytorch to build a full Seq2Seq model.\n",
    "* The first step of the architecture is to create an encoder with an LSTM unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        # Set the instance variables for the sizes and dropout probability.\n",
    "        self.input_size = input_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Initialize the embedding layer with the input size and embedding size.\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "\n",
    "        # Initialize the LSTM layer with the embedding size and hidden size,\n",
    "        # and set the number of layers and dropout probability.\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers=num_layers, dropout=dropout)\n",
    "\n",
    "    def forward(self, src_batch):\n",
    "\n",
    "        # Embed the source batch using the embedding layer.\n",
    "        embedded = self.embedding(src_batch)\n",
    "\n",
    "        # Pass the embedded batch through the LSTM layer to get the final hidden and cell states.\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "\n",
    "        # Return the final hidden and cell states.\n",
    "        return hidden, cell\n",
    "\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create the Decoder\n",
    "* The second step of the architecture is to create a decoder using a second LSTM unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, output_dim, embedding_size, hidden_size, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        # initialize the decoder class with the required parameters\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # create an embedding layer to embed the target input sequence\n",
    "        self.embedding = nn.Embedding(self.output_dim, self.embedding_size)\n",
    "\n",
    "        # create an LSTM layer with the input size as the embedding size and hidden size as the hidden size\n",
    "        # output of this LSTM layer will be the output of the decoder\n",
    "        self.lstm = nn.LSTM(self.embedding_size, self.hidden_size, num_layers=self.num_layers, dropout=self.dropout)\n",
    "\n",
    "        # create a linear layer to convert the output of the LSTM layer to the required output dimension\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_dim)\n",
    "\n",
    "    def forward(self, trg, hidden, cell):\n",
    "        # trg: target input sequence (batch size, seq len)\n",
    "        # hidden: last hidden state of the encoder (num layers * num directions, batch size, hidden size)\n",
    "        # cell: last cell state of the encoder (num layers * num directions, batch size, hidden size)\n",
    "\n",
    "        # embed the target input sequence\n",
    "        embedded = self.embedding(trg.unsqueeze(0)) # (1, batch size, embedding size)\n",
    "\n",
    "        # pass the embedded sequence through the LSTM layer along with the hidden and cell states from the encoder\n",
    "        # the output of the LSTM layer will be the hidden state and cell state of the current time step\n",
    "        outputs, (hidden, cell) = self.lstm(embedded, (hidden, cell)) \n",
    "\n",
    "        # convert the output of the LSTM layer to the required output dimension using the linear layer\n",
    "        # the prediction of the current time step will be the output of the linear layer\n",
    "        prediction = self.out(outputs.squeeze(0)) # (batch size, output dim)\n",
    "\n",
    "        # return the prediction, hidden state and cell state of the current time step\n",
    "        return prediction, hidden, cell\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Combine them into a Seq2Seq Architecture\n",
    "* To finalize our model, we will combine the encoder and decoder units into a working model.\n",
    "* The Seq2Seq2 model it's able to instantiate the encoder and decoder. Then, it will accept the inputs for these units and manage their interaction to get an output using the forward pass function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src_batch, trg_batch, teacher_forcing_ratio: float=0.5):\n",
    "\n",
    "        # get maximum sequence length and batch size\n",
    "        max_len, batch_size = trg_batch.shape\n",
    "        # get the size of the target vocabulary\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        # create tensor to store outputs\n",
    "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        # get hidden and cell state from encoder\n",
    "        hidden, cell = self.encoder(src_batch)\n",
    "\n",
    "        # use the first target token as input to the decoder\n",
    "        trg = trg_batch[0]\n",
    "        # iterate over the remaining target tokens\n",
    "        for i in range(1, max_len):\n",
    "            # pass the previous target token and hidden state to the decoder to get a prediction\n",
    "            prediction, hidden, cell = self.decoder(trg, hidden, cell)\n",
    "            # store the prediction in the outputs tensor\n",
    "            outputs[i] = prediction\n",
    "\n",
    "            # decide whether to use teacher forcing or not for the next token\n",
    "            if random.random() < teacher_forcing_ratio:\n",
    "                # use the next target token as input to the decoder\n",
    "                trg = trg_batch[i]\n",
    "            else:\n",
    "                # use the predicted token as input to the decoder\n",
    "                trg = prediction.argmax(1)\n",
    "\n",
    "        return outputs\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Train & Evaluate th model\n",
    "* Finally we will train and evaluate our model using a Pytorch training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiates an encoder, a decoder, and a Seq2Seq model.\n",
    "encoder = Encoder(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, NUM_LAYERS, DROPOUT)\n",
    "decoder = Decoder(OUTPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, NUM_LAYERS, DROPOUT)\n",
    "seq2seq = Seq2Seq(encoder, decoder, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(seq2seq.parameters(), lr=0.001)\n",
    "\n",
    "# ignore the padding index when calculating the loss\n",
    "PAD_IDX = target.vocab.stoi['<pad>']\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(seq2seq, iterator, optimizer, criterion):\n",
    "    # set the model to training mode\n",
    "    seq2seq.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    # loop through the data iterator\n",
    "    for batch in iterator:\n",
    "        \n",
    "        # reset the gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        # pass the source and target sentences through the model to get the output\n",
    "        outputs = seq2seq(batch.src, batch.trg)\n",
    "\n",
    "        # flatten the output and target sentences (excluding the start token)\n",
    "        outputs_flatten = outputs[1:].view(-1, outputs.shape[-1])\n",
    "        trg_flatten = batch.trg[1:].view(-1)\n",
    "        # compute the loss between the output and target sentences\n",
    "        loss = criterion(outputs_flatten, trg_flatten)\n",
    "\n",
    "        # compute the gradients of the loss with respect to the model parameters\n",
    "        loss.backward()\n",
    "        # update the parameters using the computed gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # add the current batch's loss to the epoch loss (scaled by the batch size)\n",
    "        epoch_loss += loss.item()*0.1\n",
    "\n",
    "    # return the average epoch loss\n",
    "    return epoch_loss / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(seq2seq, iterator, criterion):\n",
    "    # set the model to evaluation mode\n",
    "    seq2seq.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    # turn off gradients for evaluation\n",
    "    with torch.no_grad():\n",
    "        # loop through the data iterator\n",
    "        for batch in iterator:\n",
    "            # generate output using the model, with no teacher forcing\n",
    "            outputs = seq2seq(batch.src, batch.trg, teacher_forcing_ratio=0) \n",
    "            # flatten the output and target sentences (excluding the start token)\n",
    "            outputs_flatten = outputs[1:].view(-1, outputs.shape[-1])\n",
    "            trg_flatten = batch.trg[1:].view(-1)\n",
    "            # compute the loss between the output and target sentences\n",
    "            loss = criterion(outputs_flatten, trg_flatten)\n",
    "            # add the current batch's loss to the epoch loss (scaled by the batch size)\n",
    "            epoch_loss += loss.item()*0.1\n",
    "\n",
    "    # return the average epoch loss\n",
    "    return epoch_loss / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch: 0\n",
      "\tTrain Loss: 0.520 | Train PPL:   1.682\n",
      "\t Val. Loss: 0.483 |  Val. PPL:   1.621\n",
      "\n",
      " Epoch: 1\n",
      "\tTrain Loss: 0.472 | Train PPL:   1.603\n",
      "\t Val. Loss: 0.490 |  Val. PPL:   1.633\n",
      "\n",
      " Epoch: 2\n",
      "\tTrain Loss: 0.448 | Train PPL:   1.565\n",
      "\t Val. Loss: 0.468 |  Val. PPL:   1.597\n",
      "\n",
      " Epoch: 3\n",
      "\tTrain Loss: 0.427 | Train PPL:   1.533\n",
      "\t Val. Loss: 0.461 |  Val. PPL:   1.586\n",
      "\n",
      " Epoch: 4\n",
      "\tTrain Loss: 0.411 | Train PPL:   1.509\n",
      "\t Val. Loss: 0.456 |  Val. PPL:   1.578\n",
      "\n",
      " Epoch: 5\n",
      "\tTrain Loss: 0.396 | Train PPL:   1.485\n",
      "\t Val. Loss: 0.442 |  Val. PPL:   1.556\n",
      "\n",
      " Epoch: 6\n",
      "\tTrain Loss: 0.385 | Train PPL:   1.470\n",
      "\t Val. Loss: 0.433 |  Val. PPL:   1.542\n",
      "\n",
      " Epoch: 7\n",
      "\tTrain Loss: 0.373 | Train PPL:   1.452\n",
      "\t Val. Loss: 0.424 |  Val. PPL:   1.528\n",
      "\n",
      " Epoch: 8\n",
      "\tTrain Loss: 0.360 | Train PPL:   1.434\n",
      "\t Val. Loss: 0.417 |  Val. PPL:   1.517\n",
      "\n",
      " Epoch: 9\n",
      "\tTrain Loss: 0.350 | Train PPL:   1.419\n",
      "\t Val. Loss: 0.407 |  Val. PPL:   1.502\n",
      "\n",
      " Epoch: 10\n",
      "\tTrain Loss: 0.339 | Train PPL:   1.404\n",
      "\t Val. Loss: 0.398 |  Val. PPL:   1.489\n",
      "\n",
      " Epoch: 11\n",
      "\tTrain Loss: 0.330 | Train PPL:   1.390\n",
      "\t Val. Loss: 0.395 |  Val. PPL:   1.485\n",
      "\n",
      " Epoch: 12\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.379\n",
      "\t Val. Loss: 0.390 |  Val. PPL:   1.477\n",
      "\n",
      " Epoch: 13\n",
      "\tTrain Loss: 0.313 | Train PPL:   1.367\n",
      "\t Val. Loss: 0.385 |  Val. PPL:   1.469\n",
      "\n",
      " Epoch: 14\n",
      "\tTrain Loss: 0.303 | Train PPL:   1.354\n",
      "\t Val. Loss: 0.384 |  Val. PPL:   1.468\n",
      "\n",
      " Epoch: 15\n",
      "\tTrain Loss: 0.298 | Train PPL:   1.347\n",
      "\t Val. Loss: 0.378 |  Val. PPL:   1.460\n",
      "\n",
      " Epoch: 16\n",
      "\tTrain Loss: 0.289 | Train PPL:   1.335\n",
      "\t Val. Loss: 0.379 |  Val. PPL:   1.461\n",
      "\n",
      " Epoch: 17\n",
      "\tTrain Loss: 0.284 | Train PPL:   1.328\n",
      "\t Val. Loss: 0.373 |  Val. PPL:   1.452\n",
      "\n",
      " Epoch: 18\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
      "\t Val. Loss: 0.375 |  Val. PPL:   1.455\n",
      "\n",
      " Epoch: 19\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.373 |  Val. PPL:   1.452\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 20\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# loop through each epoch\n",
    "for epoch in range(N_EPOCHS):    \n",
    "    # train the model on the training data for one epoch\n",
    "    train_loss = train(seq2seq.cuda(), train_iterator, optimizer, criterion)\n",
    "    # evaluate the model on the validation data\n",
    "    valid_loss = evaluate(seq2seq, valid_iterator, criterion)\n",
    "\n",
    "    # if the current validation loss is better than the best seen so far, save the model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(seq2seq.state_dict(), 'bst-model.pt')\n",
    "    \n",
    "    # print the current epoch's loss and perplexity for both training and validation data\n",
    "    print(f'\\n Epoch: {epoch}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.374\n"
     ]
    }
   ],
   "source": [
    "# Load the saved best model state\n",
    "seq2seq.load_state_dict(torch.load('bst-model.pt'))\n",
    "\n",
    "# Evaluate the model on the test data using the loaded model state\n",
    "test_loss = evaluate(seq2seq, test_iterator, criterion)\n",
    "\n",
    "# Print the test loss using a formatted string with appropriate decimal precision\n",
    "print(f'Test Loss: {test_loss:.3f}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Interact with the System\n",
    "* Testing our system by converting the outputs of the model to text and displaying it.\n",
    "* Selects a random index from the training data, retrieves the corresponding source and target sentences, and processes them as tensors to be fed to the model.\n",
    "* The model is then put into evaluation mode, which disables dropout and other layers that may behave differently during training, and the source and target tensors are fed to the model to generate output. \n",
    "* The predicted output indices are extracted and converted to words using the target vocabulary, and the resulting words are joined into a single string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source sentence:  . gießt hemd weißen einem in mann einen auf wasser der , jacke roten einer in junge ein\n",
      "target sentence:  a boy in a red jacket pouring water on a man in a white shirt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'a boy in a red jacket is a a man with a woman in a red'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select a random index to use as an example\n",
    "sample_idx = 50\n",
    "\n",
    "# Retrieve a source and target sentence pair from the training data using the selected index\n",
    "sample = train_data.examples[sample_idx]\n",
    "\n",
    "# Print the source and target sentences using formatted strings\n",
    "print(f'source sentence: ', ' '.join(sample.src))\n",
    "print(f'target sentence: ', ' '.join(sample.trg))\n",
    "\n",
    "# Process the source and target sentences as tensors and move them to the device (GPU)\n",
    "src_tensor = source.process([sample.src]).to(device)\n",
    "trg_tensor = target.process([sample.trg]).to(device)\n",
    "\n",
    "# Set the model to evaluation mode and disable gradient computation\n",
    "seq2seq.eval()\n",
    "with torch.no_grad():\n",
    "    # Feed the source and target tensors to the model to generate output\n",
    "    outputs = seq2seq(src_tensor, trg_tensor, teacher_forcing_ratio=0)\n",
    "    \n",
    "# Extract the predicted output indices from the model output\n",
    "output_idx = outputs[1:].squeeze(1).argmax(1)\n",
    "\n",
    "# Convert the predicted output indices to words and join them into a single string\n",
    "output_sentence = ' '.join([target.vocab.itos[idx] for idx in output_idx])\n",
    "\n",
    "# Print the predicted output sentence\n",
    "print(output_sentence)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "94cefa72973aca3eba09b4ea5216a56f07737dc8d5b3858d05722ff769960377"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
